{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom LSTM/RNN AI summer experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJn7cMAYuVB",
        "colab_type": "text"
      },
      "source": [
        "# Welcome to AI Summer tutorial:Intuitive understanding of recurrent neural networks\n",
        "\n",
        "This eductional LSTM tutorial heavily borrows from the Pytorch example for time sequence prediction that can be found here: https://github.com/pytorch/examples/tree/master/time_sequence_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDhz_3AJY185",
        "colab_type": "text"
      },
      "source": [
        "### Basic imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsO7InKfWzg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuMNr4_x8OR2",
        "colab_type": "text"
      },
      "source": [
        "### Generate synthetic sin wave data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWUEulFS8R9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)\n",
        "\n",
        "T = 20\n",
        "L = 1000\n",
        "N = 200\n",
        "\n",
        "x = np.empty((N, L), 'int64')\n",
        "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
        "data = np.sin(x / 1.0 / T).astype('float64')\n",
        "torch.save(data, open('traindata.pt', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vtu67XoZBbA",
        "colab_type": "text"
      },
      "source": [
        "### Our humble implementation of LSTM cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz1VsclwW5tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LSTM_cell_AI_SUMMER(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A simple LSTM cell network for educational AI-summer purposes\n",
        "    \"\"\"\n",
        "    def __init__(self, input_length=10, hidden_length=20):\n",
        "        super(LSTM_cell_AI_SUMMER, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_length = hidden_length\n",
        "\n",
        "        # forget gate components\n",
        "        self.linear_forget_w1 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_forget_r1 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_forget = nn.Sigmoid()\n",
        "\n",
        "        # input gate components\n",
        "        self.linear_gate_w2 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r2 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_gate = nn.Sigmoid()\n",
        "\n",
        "        # cell memory components\n",
        "        self.linear_gate_w3 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r3 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.activation_gate = nn.Tanh()\n",
        "\n",
        "        # out gate components\n",
        "        self.linear_gate_w4 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r4 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_hidden_out = nn.Sigmoid()\n",
        "\n",
        "        self.activation_final = nn.Tanh()\n",
        "\n",
        "    def forget(self, x, h):\n",
        "        x = self.linear_forget_w1(x)\n",
        "        h = self.linear_forget_r1(h)\n",
        "        return self.sigmoid_forget(x + h)\n",
        "\n",
        "    def input_gate(self, x, h):\n",
        "\n",
        "        # Equation 1. input gate\n",
        "        x_temp = self.linear_gate_w2(x)\n",
        "        h_temp = self.linear_gate_r2(h)\n",
        "        i = self.sigmoid_gate(x_temp + h_temp)\n",
        "        return i\n",
        "    \n",
        "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
        "        x = self.linear_gate_w3(x)\n",
        "        h = self.linear_gate_r3(h)\n",
        "\n",
        "        # new information part that will be injected in the new context\n",
        "        k = self.activation_gate(x + h)\n",
        "        g = k * i\n",
        "        \n",
        "        # forget old context/cell info\n",
        "        c = f * c_prev\n",
        "        # learn new context/cell info\n",
        "        c_next = g + c\n",
        "        return c_next\n",
        "\n",
        "    def out_gate(self, x, h):\n",
        "        x = self.linear_gate_w4(x)\n",
        "        h = self.linear_gate_r4(h)\n",
        "        return self.sigmoid_hidden_out(x + h)\n",
        "\n",
        "    def forward(self, x, tuple_in ):\n",
        "        (h, c_prev) = tuple_in\n",
        "        # Equation 1. input gate\n",
        "        i = self.input_gate(x, h)\n",
        "        \n",
        "        # Equation 2. forget gate  \n",
        "        f = self.forget(x, h)\n",
        "\n",
        "        # Equation 3. updating the cell memory\n",
        "        c_next = self.cell_memory_gate(i, f, x, h,c_prev)\n",
        "\n",
        "        # Equation 4. calculate the main output gate\n",
        "        o = self.out_gate(x, h)\n",
        "\n",
        "      \n",
        "        # Equation 5. produce next hidden output\n",
        "        h_next = o * self.activation_final(c_next)\n",
        "\n",
        "        return h_next, c_next\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw3qlyjKZH7I",
        "colab_type": "text"
      },
      "source": [
        "### Our more humble implementation of GRU cell\n",
        "\n",
        "We will descibr GRU in part 2 but can you play around with it, if you want!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WjohnX4XB9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRU_cell_AI_SUMMER(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A simple GRU cell network for educational purposes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_length=10, hidden_length=20):\n",
        "        super(GRU_cell_AI_SUMMER, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_length = hidden_length\n",
        "\n",
        "        # reset gate components\n",
        "        self.linear_reset_w1 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_reset_r1 = nn.Linear(self.hidden_length, self.hidden_length, bias=True)\n",
        "\n",
        "\n",
        "        self.linear_reset_w2 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_reset_r2 = nn.Linear(self.hidden_length, self.hidden_length, bias=True)\n",
        "        self.activation_1 = nn.Sigmoid()\n",
        "\n",
        "        # update gate components\n",
        "        self.linear_gate_w3 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r3 = nn.Linear(self.hidden_length, self.hidden_length, bias=True)\n",
        "        self.activation_2 = nn.Sigmoid()\n",
        "\n",
        "        self.activation_3 = nn.Tanh()\n",
        "\n",
        "    def reset_gate(self, x, h):\n",
        "        x_1 = self.linear_reset_w1(x)\n",
        "        h_1 = self.linear_reset_r1(h)\n",
        "        # gate update\n",
        "        reset = self.activation_1(x_1 + h_1)\n",
        "        return reset\n",
        "\n",
        "    def update_gate(self, x, h):\n",
        "        x_2 = self.linear_reset_w2(x)\n",
        "        h_2 = self.linear_reset_r2(h)\n",
        "        z = self.activation_2( h_2 + x_2)\n",
        "        return z\n",
        "\n",
        "    \n",
        "    def update_component(self, x,h,r):\n",
        "        x_3 = self.linear_gate_w3(x)\n",
        "        h_3 = r * self.linear_gate_r3(h) \n",
        "        gate_update = self.activation_3(x_3+h_3)\n",
        "        return gate_update\n",
        "\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Equation 1. reset gate vector\n",
        "        r = self.reset_gate(x, h)\n",
        "\n",
        "        # Equation 2: the update gate - the shared update gate vector z\n",
        "        z = self.update_gate(x, h)\n",
        "\n",
        "        # Equation 3: The almost output component\n",
        "        n = self.update_component(x,h,r)\n",
        "\n",
        "        # Equation 4: the new hidden state\n",
        "        h_new = (1-z) * n  + z * h\n",
        "\n",
        "        return h_new\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMgHiLofZfsF",
        "colab_type": "text"
      },
      "source": [
        "## Putting the cells together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xyt-Gx8XHtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequence(nn.Module):\n",
        "    def __init__(self, LSTM=True, custom=True):\n",
        "        super(Sequence, self).__init__()\n",
        "        self.LSTM = LSTM\n",
        "\n",
        "        if LSTM:\n",
        "          if custom:\n",
        "            print(\"AI summer LSTM cell implementation...\")\n",
        "            self.rnn1 = LSTM_cell_AI_SUMMER(1, 51)\n",
        "            self.rnn2 = LSTM_cell_AI_SUMMER(51, 51)\n",
        "          else:\n",
        "            print(\"Official PyTorch LSTM cell implementation...\")\n",
        "            self.rnn1 = nn.LSTMCell(1, 51)\n",
        "            self.rnn2 = nn.LSTMCell(51, 51)\n",
        "        #GRU\n",
        "        else:\n",
        "          if custom:\n",
        "            print(\"AI summer GRU cell implementation...\")\n",
        "            self.rnn1 = GRU_cell_AI_SUMMER(1, 51)\n",
        "            self.rnn2 = GRU_cell_AI_SUMMER(51, 51)\n",
        "          else:\n",
        "            print(\"Official PyTorch GRU cell implementation...\")\n",
        "            self.rnn1 = nn.GRUCell(1, 51)\n",
        "            self.rnn2 = nn.GRUCell(51, 51)\n",
        "\n",
        "\n",
        "        self.linear = nn.Linear(51, 1)\n",
        "\n",
        "    def forward(self, input, future=0):\n",
        "        outputs = []\n",
        "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
        "\n",
        "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
        "\n",
        "            if self.LSTM:\n",
        "              h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
        "              h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
        "            else:\n",
        "              h_t = self.rnn1(input_t, h_t)\n",
        "              h_t2 = self.rnn2(h_t, h_t2)\n",
        "\n",
        "            output = self.linear(h_t2)\n",
        "            outputs += [output]\n",
        "        \n",
        "        # if we should predict the future\n",
        "        for i in range(future):  \n",
        "            if self.LSTM:\n",
        "              h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
        "              h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
        "            else:\n",
        "              h_t = self.rnn1(input_t, h_t)\n",
        "              h_t2 = self.rnn2(h_t, h_t2)\n",
        "\n",
        "            output = self.linear(h_t2)\n",
        "            outputs += [output]\n",
        "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEstEw_uU9n2",
        "colab_type": "text"
      },
      "source": [
        "## Train code (based on the Pytorch example for time sequence prediction)\n",
        "\n",
        "that can be found here: https://github.com/pytorch/examples/tree/master/time_sequence_prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA5kieEMXY4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61d8380b-41ca-480e-8271-418ffec8b746"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # set random seed to 0\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    # load data and make training set\n",
        "    data = torch.load('traindata.pt')\n",
        "    input = torch.from_numpy(data[3:, :-1])\n",
        "    print(input.shape)\n",
        "    target = torch.from_numpy(data[3:, 1:])\n",
        "    test_input = torch.from_numpy(data[:3, :-1])\n",
        "    test_target = torch.from_numpy(data[:3, 1:])\n",
        "    \n",
        "    # build the model. LSTM=False means GRU cell\n",
        "    seq = Sequence(LSTM=True, custom=True)\n",
        "\n",
        "    seq.double()\n",
        "    criterion = nn.MSELoss()\n",
        "    # use LBFGS as optimizer since we can load the whole data to train\n",
        "    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
        "    # begin to train\n",
        "    for i in range(20):\n",
        "        print('STEP: ', i)\n",
        "\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            out = seq(input)\n",
        "            loss = criterion(out, target)\n",
        "            print('loss:', loss.item())\n",
        "            loss.backward()\n",
        "            return loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        # begin to predict, no need to track gradient here\n",
        "        with torch.no_grad():\n",
        "            future = 1000\n",
        "            pred = seq(test_input, future=future)\n",
        "            loss = criterion(pred[:, :-future], test_target)\n",
        "            print('test loss:', loss.item())\n",
        "            y = pred.detach().numpy()\n",
        "        # draw the result\n",
        "        plt.figure(figsize=(30, 10))\n",
        "        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
        "        plt.xlabel('x', fontsize=20)\n",
        "        plt.ylabel('y', fontsize=20)\n",
        "        plt.xticks(fontsize=20)\n",
        "        plt.yticks(fontsize=20)\n",
        "\n",
        "\n",
        "        def draw(yi, color):\n",
        "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth=2.0)\n",
        "            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth=2.0)\n",
        "\n",
        "\n",
        "        draw(y[0], 'r')\n",
        "        draw(y[1], 'g')\n",
        "        draw(y[2], 'b')\n",
        "        plt.savefig('predict%d.png' % i)\n",
        "        plt.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([197, 999])\n",
            "AI summer LSTM cell implementation...\n",
            "STEP:  0\n",
            "loss: 0.528113070229433\n",
            "loss: 0.5170091480952497\n",
            "loss: 0.9555554711921114\n",
            "loss: 0.02929956013377312\n",
            "loss: 0.02668833057236316\n",
            "loss: 0.02578294447759208\n",
            "loss: 0.024732702950737373\n",
            "loss: 0.022838443305267297\n",
            "loss: 0.019883604722129907\n",
            "loss: 0.014934575438973216\n",
            "loss: 0.01294144748634683\n",
            "loss: 0.01125866273027107\n",
            "loss: 0.010340490329751066\n",
            "loss: 0.005540535925217467\n",
            "loss: 0.003720403467427374\n",
            "loss: 0.002060538268350741\n",
            "loss: 0.0014695789705200751\n",
            "loss: 0.0008976667789292632\n",
            "loss: 0.0005588928839437537\n",
            "loss: 0.000553251786021077\n",
            "test loss: 0.00025937913340416294\n",
            "STEP:  1\n",
            "loss: 0.0004188821271592212\n",
            "loss: 0.0004115594020828428\n",
            "loss: 0.0004098080070695498\n",
            "loss: 0.00040585335549792625\n",
            "loss: 0.00039800889556492565\n",
            "loss: 0.0003823670322268507\n",
            "loss: 0.0003565621034820429\n",
            "loss: 0.0003271524037103022\n",
            "loss: 0.00031091980677155646\n",
            "loss: 0.00030442897653582625\n",
            "loss: 0.00030135321920862106\n",
            "loss: 0.000297159804392728\n",
            "loss: 0.00028623982689565544\n",
            "loss: 0.00026367783932863267\n",
            "loss: 0.00023431325251627166\n",
            "loss: 0.000198653027681383\n",
            "loss: 0.0001815602801224594\n",
            "loss: 0.00017243991942617054\n",
            "loss: 0.000167400305207888\n",
            "loss: 0.00016676000062173038\n",
            "test loss: 8.630216931841515e-05\n",
            "STEP:  2\n",
            "loss: 0.0001655738129420653\n",
            "loss: 0.00016274375068161108\n",
            "loss: 0.0001594271200927448\n",
            "loss: 0.00015566651539603154\n",
            "loss: 0.00015310920266744678\n",
            "loss: 0.00015175200303974804\n",
            "loss: 0.00015021718169219833\n",
            "loss: 0.00014764850590202232\n",
            "loss: 0.00014406411552009922\n",
            "loss: 0.00014020286381351805\n",
            "loss: 0.00013721598097735975\n",
            "loss: 0.00013564592536166646\n",
            "loss: 0.0001141601658816323\n",
            "loss: 9.632621856127124e-05\n",
            "loss: 8.540454699156185e-05\n",
            "loss: 8.00886269558638e-05\n",
            "loss: 7.866357692138397e-05\n",
            "loss: 7.846270683516499e-05\n",
            "loss: 7.803720795279226e-05\n",
            "loss: 7.787795173710368e-05\n",
            "test loss: 5.309506777407865e-05\n",
            "STEP:  3\n",
            "loss: 7.758916682904886e-05\n",
            "loss: 7.72278791969809e-05\n",
            "loss: 7.66369668169787e-05\n",
            "loss: 7.512375054800206e-05\n",
            "loss: 6.960300053678313e-05\n",
            "loss: 5.974397824269719e-05\n",
            "loss: 5.285812496968241e-05\n",
            "loss: 5.879130528044276e-05\n",
            "loss: 4.5344381081473816e-05\n",
            "loss: 4.428458576933465e-05\n",
            "loss: 4.289067757161094e-05\n",
            "loss: 4.234652194868516e-05\n",
            "loss: 4.2188541758263925e-05\n",
            "loss: 4.211897040355653e-05\n",
            "loss: 4.208079302286101e-05\n",
            "loss: 4.20530790474463e-05\n",
            "loss: 4.201103741671751e-05\n",
            "loss: 4.171172915792721e-05\n",
            "loss: 4.023048477710445e-05\n",
            "loss: 3.8600785264667686e-05\n",
            "test loss: 2.0303105849979015e-05\n",
            "STEP:  4\n",
            "loss: 3.655464978675086e-05\n",
            "loss: 3.4447083416544624e-05\n",
            "loss: 3.2380408661129566e-05\n",
            "loss: 3.1224058027911596e-05\n",
            "loss: 2.982218832569802e-05\n",
            "loss: 2.9339563104859834e-05\n",
            "loss: 2.8842857399152147e-05\n",
            "loss: 2.8621022762785623e-05\n",
            "loss: 2.8250528581550616e-05\n",
            "loss: 2.7601681645153186e-05\n",
            "loss: 2.666863185511221e-05\n",
            "loss: 2.6065789274730817e-05\n",
            "loss: 2.5220270154457496e-05\n",
            "loss: 2.4755895857374163e-05\n",
            "loss: 2.4459466972747967e-05\n",
            "loss: 2.4289890236020474e-05\n",
            "loss: 2.417871460234937e-05\n",
            "loss: 2.405552084448569e-05\n",
            "loss: 2.3803825009267305e-05\n",
            "loss: 2.3391986899631617e-05\n",
            "test loss: 1.0651709515802253e-05\n",
            "STEP:  5\n",
            "loss: 2.2722787639648107e-05\n",
            "loss: 2.195350374047056e-05\n",
            "loss: 2.0790937997244048e-05\n",
            "loss: 2.0559415845903276e-05\n",
            "loss: 1.8719478798203408e-05\n",
            "loss: 1.775920559743769e-05\n",
            "loss: 1.6894070432816332e-05\n",
            "loss: 1.4953226300362585e-05\n",
            "loss: 1.3812765688017387e-05\n",
            "loss: 1.2995846153363898e-05\n",
            "loss: 1.161801037515683e-05\n",
            "loss: 1.0840213562266476e-05\n",
            "loss: 1.0552324119695516e-05\n",
            "loss: 1.0364055093154938e-05\n",
            "loss: 1.019954593444896e-05\n",
            "loss: 1.0006838183569344e-05\n",
            "loss: 9.84015970834103e-06\n",
            "loss: 9.512266385666233e-06\n",
            "loss: 9.043037858738418e-06\n",
            "loss: 8.695631981398065e-06\n",
            "test loss: 8.771404789806152e-06\n",
            "STEP:  6\n",
            "loss: 8.351934709219552e-06\n",
            "loss: 8.099359845597974e-06\n",
            "loss: 8.023492137734603e-06\n",
            "loss: 7.982697316393027e-06\n",
            "loss: 7.934128231570328e-06\n",
            "loss: 7.882304943013427e-06\n",
            "loss: 7.817944812735015e-06\n",
            "loss: 7.708381470080016e-06\n",
            "loss: 7.545262800637035e-06\n",
            "loss: 7.3590306414342304e-06\n",
            "loss: 7.187149637024493e-06\n",
            "loss: 7.041607611668718e-06\n",
            "loss: 6.880853713534134e-06\n",
            "loss: 6.64904209907784e-06\n",
            "loss: 6.349631631734573e-06\n",
            "loss: 6.083386331472478e-06\n",
            "loss: 5.840510084289589e-06\n",
            "loss: 5.641808262040041e-06\n",
            "loss: 5.550992309084652e-06\n",
            "loss: 5.476518943945725e-06\n",
            "test loss: 7.558627393520033e-06\n",
            "STEP:  7\n",
            "loss: 5.424637104614732e-06\n",
            "loss: 5.343169328026148e-06\n",
            "loss: 5.278998892833016e-06\n",
            "loss: 5.233559719895183e-06\n",
            "loss: 5.1790714520945995e-06\n",
            "loss: 5.102813685644669e-06\n",
            "loss: 5.057816101579782e-06\n",
            "loss: 5.021956007848406e-06\n",
            "loss: 5.010760587860601e-06\n",
            "loss: 4.984458411239595e-06\n",
            "loss: 4.971447417822076e-06\n",
            "loss: 4.962694540121956e-06\n",
            "loss: 4.955367134554158e-06\n",
            "loss: 4.941791844369365e-06\n",
            "loss: 4.886708029358563e-06\n",
            "loss: 4.8298561848093285e-06\n",
            "loss: 4.729860370084668e-06\n",
            "loss: 4.603941070965755e-06\n",
            "loss: 4.510229859050638e-06\n",
            "loss: 4.451572626333044e-06\n",
            "test loss: 6.719516757378004e-06\n",
            "STEP:  8\n",
            "loss: 4.410182307722153e-06\n",
            "loss: 4.383787406300294e-06\n",
            "loss: 4.373314104222076e-06\n",
            "loss: 4.366633444691338e-06\n",
            "loss: 4.365811584282103e-06\n",
            "test loss: 6.687451260034044e-06\n",
            "STEP:  9\n",
            "loss: 4.365811584282103e-06\n",
            "loss: 4.364949992640159e-06\n",
            "test loss: 6.680560301308882e-06\n",
            "STEP:  10\n",
            "loss: 4.364949992640159e-06\n",
            "loss: 4.3634225594267824e-06\n",
            "loss: 4.361092433937902e-06\n",
            "loss: 4.354063689071793e-06\n",
            "loss: 4.340617338459655e-06\n",
            "loss: 4.313205800087283e-06\n",
            "loss: 4.259797513366939e-06\n",
            "loss: 4.178709907934998e-06\n",
            "loss: 4.082023948515665e-06\n",
            "loss: 4.296541790315014e-06\n",
            "loss: 3.898617119222726e-06\n",
            "loss: 3.863048014958528e-06\n",
            "loss: 3.828496011390211e-06\n",
            "loss: 3.7981087020646685e-06\n",
            "loss: 3.7854740901244747e-06\n",
            "loss: 3.774291574154713e-06\n",
            "loss: 3.7639566498014944e-06\n",
            "loss: 3.754666197950809e-06\n",
            "loss: 3.744645470054434e-06\n",
            "loss: 3.7340472375547916e-06\n",
            "test loss: 5.650664426590775e-06\n",
            "STEP:  11\n",
            "loss: 3.7271341540426344e-06\n",
            "loss: 3.7242201571422833e-06\n",
            "loss: 3.721707122961544e-06\n",
            "loss: 3.7209966604878027e-06\n",
            "test loss: 5.674726447039287e-06\n",
            "STEP:  12\n",
            "loss: 3.7209966604878027e-06\n",
            "test loss: 5.674726447039287e-06\n",
            "STEP:  13\n",
            "loss: 3.7209966604878027e-06\n",
            "test loss: 5.674726447039287e-06\n",
            "STEP:  14\n",
            "loss: 3.7209966604878027e-06\n",
            "test loss: 5.674726447039287e-06\n",
            "STEP:  15\n",
            "loss: 3.7209966604878027e-06\n",
            "test loss: 5.674726447039287e-06\n",
            "STEP:  16\n",
            "loss: 3.7209966604878027e-06\n",
            "test loss: 5.674726447039287e-06\n",
            "STEP:  17\n",
            "loss: 3.7209966604878027e-06\n",
            "test loss: 5.674726447039287e-06\n",
            "STEP:  18\n",
            "loss: 3.7209966604878027e-06\n",
            "test loss: 5.674726447039287e-06\n",
            "STEP:  19\n",
            "loss: 3.7209966604878027e-06\n",
            "test loss: 5.674726447039287e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMrOc2C6DJlQ",
        "colab_type": "text"
      },
      "source": [
        "## Zip files to download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drjk_eov-669",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "4474b0e3-ae47-4fb9-b533-4bc329cf1865"
      },
      "source": [
        "!zip archive.zip predict*.png"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: predict0.png (deflated 7%)\n",
            "  adding: predict10.png (deflated 7%)\n",
            "  adding: predict11.png (deflated 7%)\n",
            "  adding: predict12.png (deflated 7%)\n",
            "  adding: predict13.png (deflated 7%)\n",
            "  adding: predict14.png (deflated 7%)\n",
            "  adding: predict15.png (deflated 7%)\n",
            "  adding: predict16.png (deflated 7%)\n",
            "  adding: predict17.png (deflated 7%)\n",
            "  adding: predict18.png (deflated 7%)\n",
            "  adding: predict19.png (deflated 7%)\n",
            "  adding: predict1.png (deflated 7%)\n",
            "  adding: predict2.png (deflated 7%)\n",
            "  adding: predict3.png (deflated 7%)\n",
            "  adding: predict4.png (deflated 7%)\n",
            "  adding: predict5.png (deflated 7%)\n",
            "  adding: predict6.png (deflated 7%)\n",
            "  adding: predict7.png (deflated 7%)\n",
            "  adding: predict8.png (deflated 7%)\n",
            "  adding: predict9.png (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}